{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\" color=\"brown\">Author : UTSAV AWASTHI</font><br>\n",
    "<font size=\"1\" color=\"brown\">About Me: http://iamawesome.com/ </font> <br>\n",
    "Subscribe to my YouTube channel <a href=\"https://www.youtube.com/channel/UCJpp55YcP5swWj8TmIOd0OA/\">here</a><br> \n",
    "For serious conversations related to career and jobs, follow me on <a href=\"https://www.linkedin.com/in/utsavawasthi/\">LinkedIn</a><br>\n",
    "For low-effort dank memes and one-liners, follow me on <a href=\"https://twitter.com/utsavsaidwhat\">Twitter</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <font size = 3 color=\"orange\" style=\"font-family: 'Comic Sans MS'\">// ज्ञानं परमं बलम् //</font><br>\n",
    "</div>\n",
    "<img src = \"https://cloudblogs.microsoft.com/uploads/prod/sites/32/2020/05/SQL.png\" width=\"30%\" height=\"30%\">\n",
    "<br>\n",
    "\n",
    "<div align=\"center\" id=\"contents\"> <font size=\"5\" color=\"orange\"><b>STRUCTURE QUERY LANGUAGE</b></font> </div>\n",
    "<br>\n",
    "\n",
    "<div align=\"left\"> <font size=\"4\" color=\"red\"><b>TABLE OF CONTENTS</b></font> </div>\n",
    "<ol type=\"square\">\n",
    "    <li><a href=\"#intro\">Introduction</a></li>\n",
    "    <li><a href=\"#basics\">Basics</a></li>\n",
    "    <li><a href=\"#agg\">Aggregation</a></li>\n",
    "    <li><a href=\"#joins\">Joins</a></li>\n",
    "    <li><a href=\"#adv\">Advanced</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id=\"intro\"><a href=\"#contents\"><font size=\"4\" color=\"red\"><b>INTRODUCTION</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\"Data is the new Oil\"</b><br>\n",
    "You might have heard that statement quite a few times. World is increasingly generating more and more data every second. Did you know that as of right now, in one second, more than 10 hours of video are being uploaded to YouTube, 9190 Tweets are being sent out on Twitter, 1026 Instagram photos are being uploaded on Instagram and Google processes over 40,000 search queries. As you can see there are tons of different types of data which gets generated - video, text, image, transaction etc. And this is increasing exponentially with every year as the world is becoming more and more digital. Tools like SQL help us in getting meaningful insights out of this huge mountain of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Structured vs Unstructured Data</b><br>\n",
    "Something like a video or image or text can be easily understood by humans in multiple contexts. For example we might easily be able to say who is there in a picture and also describe the background in detail. Or we can read a book like Lord of the Rings and can easily understand the storyline and appreciate it. But for Computers it might be a little hard to understand data in such \"unstructured\" formats. \n",
    "On the other hand computers can easily understand data which is structured. They can exceptionally fast in getting meanining summary out of structured datasests like a table of transactions and all related information to each transaction. You can quickly calcuate mean/median/mode and multiple other things of different columns in transaction data very easily using computers. SQL can easily help you manipulate any structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Structured Query Language</b><br>\n",
    "For any analytics professional, Structured Query Language i.e. SQL is the most basic and a must have skill.\n",
    "Almost all the companies store their data in databases which are generally accessed by SQL. Unlike Python, SQL is not versatile or general purpose as it can only be used to manipulate data. And SQL is pretty powerful at manipulating data.<br>\n",
    "There are many dialects of SQL e.g. MySQL, T-SQL, PL/SQL, Hive SQL etc. Each of these dialects have most things in common, except for few syntax differences here and there. Depending on which software you are using for storing data(e.g. Hadoop/Azure/SAS/Teradata etc), you will be using different SQL dialects inside different softwares.<br>\n",
    "To keep things simple, here we will run SQL inside Python using pandasql module. We will highlight things which are currently not supported by pandasql module, but are by many other widely known SQL dialects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id=\"basics\"><a href=\"#contents\"><font size=\"4\" color=\"red\"><b>BASICS</b></font></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we will be running SQL within Python. Let's start with installing \"pandasql\" module. Just press Ctrl+Enter in each of the boxes to execute the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandasql as ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this module we will be working with Titanic dataset. It is a famous dataset which has the data of Titanic Disaster that happened in 1912. It records Name, Age, Gender, Ticket etc of each passenger and also has a flag saying if a person survived the disaster or not. To know more about this dataset <a href=\"https://www.kaggle.com/c/titanic\">click here</a>.<br>\n",
    "We will start with importing this data in pandas and then execute the SQL queries in pandasql. Do not worry if you do not know pandas. All you need to focus on is the SQL code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_disaster_dataset = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "print(titanic_disaster_dataset.shape, titanic_disaster_dataset[['PassengerId']].drop_duplicates().shape)\n",
    "titanic_disaster_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin. The SQL code is the one written in between three double quotes. This is what you should focus on and learn. <br><br>\n",
    "\n",
    "In SQL, You can select everything from a dataset using \"*\" (star symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Selecting everything from dataset\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    * \n",
    "FROM titanic_disaster_dataset\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practise to give small aliases to datasets or columns which have very long names e.g. \"titanic_disaster_dataset\". This is done using \"AS\" keyword.\n",
    "<br>\n",
    "Further you can add this alias name before the columns you are choosing from this dataset.  This makes things easier to understand for yourself and for anyone new reading your code because it is easy to see from which dataset each column is coming from. Let's see an example - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Giving an alias name to the dataset and using it in \"SELECT\" statement too\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.* \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see the usefulness of giving alias names later when we learn JOINs, where we have more than one table.\n",
    "\n",
    "Make it a habit to give alias names to all datasets you are using and also mentioning the correct alias of the dataset ahead of each columns you are taking in \"SELECT\" statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some SQL languages, you can select top n rows too. For example in Hive SQL and in pandasql, you can use \"LIMIT\" keyword in the end as described below to choose only top n rows - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using \"LIMIT\" to get top 5 rows\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.*  \n",
    "FROM titanic_disaster_dataset AS df\n",
    "LIMIT 5\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Teradata SQL, you can use \"TOP\" keyword to do the same thing. The difference is that \"TOP\" is used in the \"SELECT\" part of the query, and not at the end. An example syntax is shown below, although it won't work because unfortunately pandasql does not support \"TOP\" keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An example of using \"TOP\" to get top 5 rows.\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    TOP 3 *  \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "\"\"\")\n",
    "#Gives error because \"TOP\" is not supported in pandasql "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can choose specific columns, if you don't want everything. Simply specify the columns you want. Notice again how we are using alias names for the columns which are being chosen from titanic dataset. We will use \"LIMIT\" to avoid flooding the screen with full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Taking only Passenger ID and the name from the dataset\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.PassengerId,\n",
    "    df.Name\n",
    "FROM titanic_disaster_dataset AS df\n",
    "LIMIT 5\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also rename columns using \"AS\" keyword. Here is an example-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Taking only Passenger ID and name from the dataset and also renaming the two chosen columns\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.PassengerId AS ID_of_Passenger,\n",
    "    df.Name AS Name_of_Passenger\n",
    "FROM titanic_disaster_dataset AS df\n",
    "LIMIT 5\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>WHERE</b><br>\n",
    "If you want only specific rows based on a condition, you can mention these conditions in \"WHERE\" clause. <br>\n",
    "\"WHERE\" works row by row e.g. if the condition is Age = 10, in each row the query will check if Age is equal to 10 and if yes, then it will take that row, otherwise not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Selecting everything from the dataset for passengers whose age is below 1 year\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.*\n",
    "FROM titanic_disaster_dataset AS df\n",
    "WHERE df.age < 1\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NULL keyword</b>\n",
    "To find mising values in the data, we can use \"NULL\" keyword. Just be mindful that NULL works with \"IS\" and not with \"=\". For example, \"Age IS NULL\" is the correct way of finding nulls in Age column and \"Age = NULL\" is not (and will throw error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#You can check for missing values by using \"IS NULL\"\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.*\n",
    "FROM titanic_disaster_dataset AS df\n",
    "WHERE df.age IS NULL\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can get non-null rows by using \"IS NOT NULL\"\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.*\n",
    "FROM titanic_disaster_dataset AS df\n",
    "WHERE df.age IS NOT NULL\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use \"BETWEEN\" to filter numerical values between two numbers. Here is an example - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Example for \"BETWEEN\" keyword\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.*\n",
    "FROM titanic_disaster_dataset AS df\n",
    "WHERE df.age BETWEEN 5 AND 10\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify multiple conditions using \"AND\" or \"OR\" operators. Here are some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting everything from the dataset for passengers whose age is below 1 year and who are females\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.*\n",
    "FROM titanic_disaster_dataset AS df\n",
    "WHERE df.age < 1\n",
    "    AND df.Sex = \"female\"\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting everything from the dataset for passengers where (age is below 1 year and sex is female) OR\n",
    "#(age is above 70 year and Pclass is 1)\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.*\n",
    "FROM titanic_disaster_dataset AS df\n",
    "WHERE (df.age < 1 AND df.Sex = \"female\")\n",
    "    OR (df.age > 70 AND df.Pclass = 1)\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using \"AND\" and \"OR\" together might be tricky at times. It is best to use parantheses to enforce which conditions will be evaluated together. E.g. in the above example we have four conditions in total. First the two conditions inside the parantheses will be evaluated separately using \"AND\". And then the output of those two will be evaluated using \"OR\" statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are multiple values you want to filter in a column, you can use \"IN\" keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting everything from the dataset for passengers where Cabin is one of A5, A6, A7\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.*\n",
    "FROM titanic_disaster_dataset AS df\n",
    "WHERE df.Cabin IN ('A5', 'A6', 'A7')\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting everything from the dataset for passengers where Age is one of 65, 70, 71\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.*\n",
    "FROM titanic_disaster_dataset AS df\n",
    "WHERE df.Age IN (65, 70, 71)\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text/string columns, a powerful way of filtering is using \"LIKE\" keyword. It works by the specifying the substring you want in a string and surrounding the substring with '%'s. Here is how it looks with examples below -\n",
    "<ul>\n",
    "    <li>For Names starting with \"ABC\" - Name LIKE \"ABC%\"</li>\n",
    "    <li>For Names ending with \"ABC\" - Name LIKE \"%ABC\"</li>\n",
    "    <li>For Names which contain \"ABC\" - Name LIKE \"%ABC%\"</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting everything from the dataset for passengers where name contains 'Mrs.'\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.*\n",
    "FROM titanic_disaster_dataset AS df\n",
    "WHERE df.Name LIKE \"%MRS.%\"\n",
    "LIMIT 5\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more, the \"LIMIT\" part in the end is just to ensure that only the first few rows are displayed in the output. This avoids flooding the screen with whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the \"LIKE\" condition is not case sensitive i.e. both 'mrs.' and 'MRS.' will be found with the same search string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create new columns out of existing columns. Here are some examples-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Adding a column for Square, Log, Root etc of \n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Age,\n",
    "    df.Age * df.Age AS age_sq,\n",
    "    df.Age + 1 AS age_plus_one\n",
    "FROM titanic_disaster_dataset AS df\n",
    "LIMIT 5    \n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can concatenate two string columns using double pipe || "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating Sex and Ticket\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Sex,\n",
    "    df.Ticket,\n",
    "    df.Sex || df.Ticket AS Sex_Plus_ticket\n",
    "FROM titanic_disaster_dataset AS df\n",
    "LIMIT 5    \n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>CASE WHEN</b><br>\n",
    "You can use CASE WHEN to create new columns based on the specified conditions. Here is an example of the same -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using CASE WHEN to create buckets for age\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Name,\n",
    "    df.PassengerID,\n",
    "    df.Age,\n",
    "    CASE WHEN df.Age IS NULL THEN '0. Missing'\n",
    "        WHEN df.Age <18 THEN '1. 1-17'\n",
    "        WHEN df.Age <60 THEN '2. 18-60'\n",
    "    ELSE '3. 60+' END AS age_bucket\n",
    "FROM titanic_disaster_dataset AS df\n",
    "LIMIT 50   \n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SUBSTR()</b> function can be used to take out a part of a string from the text columns in a dataset. SUBSTR() function requires three inputs - SUBSTR(<i>column_name, starting_point, number_of_characters</i>). Here is an example where we output 4 characters, starting from the second character from \"Name\" column - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SUBSTR() to take out a substring\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Name,\n",
    "    SUBSTR(df.Name,2, 4) AS out\n",
    "FROM titanic_disaster_dataset AS df\n",
    "LIMIT 5\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LOWER() and UPPER()</b> can be used to convert text columns to lower and upper case, respectively. Here is an example -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using LOWER() and UPPER() to convert text to lower and upper case\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Name,\n",
    "    LOWER(df.Name) AS lcase_name,\n",
    "    UPPER(df.Name) AS ucase_name\n",
    "FROM titanic_disaster_dataset AS df\n",
    "LIMIT 5\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get all the unique values in a column using \"DISTINCT\" keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all distinct values of \"Embarked\" column\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    DISTINCT df.Embarked\n",
    "FROM titanic_disaster_dataset AS df\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id=\"agg\"><a href=\"#contents\"><font size=\"4\" color=\"red\"><b>AGGREGATION</b></font></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL provides multiple ways in which you can aggregate information at different levels. You will need to use \"GROUP BY\" in the end and specify on which column(s) you are aggregating the data. <br>\n",
    "Here are some examples -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating information at \"Embarked\" column level\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Embarked,\n",
    "    AVG(df.Age) AS avg_age,\n",
    "    SUM(df.Age) AS tot_age,\n",
    "    MIN(df.Age) AS min_age,\n",
    "    MAX(df.Age) AS max_age,\n",
    "    COUNT(df.PassengerId) AS tot_passengers,\n",
    "    COUNT(DISTINCT df.Ticket) AS dist_tickets\n",
    "FROM titanic_disaster_dataset AS df\n",
    "GROUP BY df.Embarked\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the aggregate functions used above, many SQL dialects also support STDEV for standard deviation, VAR for variance, SQRT for square root and many more. You can always google to find out which aggregate functions are supported by the SQL dialect you are using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating information at \"Embarked\" and \"Sex\" levels\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    AVG(df.Age) AS avg_age,\n",
    "    SUM(df.Age) AS tot_age,\n",
    "    MIN(df.Age) AS min_age,\n",
    "    MAX(df.Age) AS max_age,\n",
    "    COUNT(df.PassengerId) AS tot_passengers,\n",
    "    COUNT(DISTINCT df.Ticket) AS dist_tickets\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "GROUP BY df.Embarked,\n",
    "    df.Sex\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating information at \"Embarked\" and \"Sex\" levels \n",
    "#and also using \"WHERE\" to filter basis some conditions\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    AVG(df.Age) AS avg_age,\n",
    "    SUM(df.Age) AS tot_age,\n",
    "    MIN(df.Age) AS min_age,\n",
    "    MAX(df.Age) AS max_age,\n",
    "    COUNT(df.PassengerId) AS tot_passengers,\n",
    "    COUNT(DISTINCT df.Ticket) AS dist_tickets\n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "WHERE age >= 18\n",
    "    AND age <=60\n",
    "    \n",
    "GROUP BY df.Embarked,\n",
    "    df.Sex\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"WHERE\" is always used before \"GROUP BY\" <br>\n",
    "You can also sort the output using \"ORDER BY\" in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating information at \"Embarked\" and \"Sex\" levels \n",
    "#and also using \"WHERE\" to filter basis some conditions\n",
    "#and also sorting the output\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    AVG(df.Age) AS avg_age,\n",
    "    SUM(df.Age) AS tot_age,\n",
    "    MIN(df.Age) AS min_age,\n",
    "    MAX(df.Age) AS max_age,\n",
    "    COUNT(df.PassengerId) AS tot_passengers,\n",
    "    COUNT(DISTINCT df.Ticket) AS dist_tickets\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "WHERE age >= 18\n",
    "    AND age <=60\n",
    "    \n",
    "GROUP BY df.Embarked,\n",
    "    df.Sex\n",
    "\n",
    "ORDER BY df.Embarked,\n",
    "    df.Sex\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the sorting is done in Ascending order but you can change that to descending order by using \"DESC\" keyword in \"ORDER BY\" statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating information at \"Embarked\" and \"Sex\" levels \n",
    "#and also using \"WHERE\" to filter basis some conditions\n",
    "#and also sorting the output in descending order\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    AVG(df.Age) AS avg_age,\n",
    "    SUM(df.Age) AS tot_age,\n",
    "    MIN(df.Age) AS min_age,\n",
    "    MAX(df.Age) AS max_age,\n",
    "    COUNT(df.PassengerId) AS tot_passengers,\n",
    "    COUNT(DISTINCT df.Ticket) AS dist_tickets\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "WHERE age >= 18\n",
    "    AND age <=60\n",
    "    \n",
    "GROUP BY df.Embarked,\n",
    "    df.Sex\n",
    "\n",
    "ORDER BY df.Embarked DESC,\n",
    "    df.Sex DESC\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> HAVING </b><br>\n",
    "Lastly, once you have aggregated the information, you can further put filters on the aggregated output using \"HAVING\". Always remember the difference between \"WHERE\" and \"HAVING\" as it is one of the most commonly asked interview questions. \"WHERE\" is used to filter the data directly from the tables while \"HAVING\" is used to filter the aggregated data you get as the output of a SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregating information at \"Embarked\" and \"Sex\" levels \n",
    "#and also using \"WHERE\" to filter basis some conditions\n",
    "#and also using \"HAVING\" for fitering the output for buckets where average age is more than 30\n",
    "#and also using \"ORDER BY\" for sorting the output in descending order\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    AVG(df.Age) AS avg_age,\n",
    "    SUM(df.Age) AS tot_age,\n",
    "    MIN(df.Age) AS min_age,\n",
    "    MAX(df.Age) AS max_age,\n",
    "    COUNT(df.PassengerId) AS tot_passengers,\n",
    "    COUNT(DISTINCT df.Ticket) AS dist_tickets\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "WHERE age >= 18\n",
    "    AND age <=60\n",
    "    \n",
    "GROUP BY df.Embarked,\n",
    "    df.Sex\n",
    "\n",
    "HAVING avg_age > 30\n",
    "\n",
    "ORDER BY df.Embarked DESC,\n",
    "    df.Sex DESC\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Remember the order - First we used \"WHERE\", then \"GROUP BY\", then \"HAVING\" and lastly \"ORDER BY\"</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id=\"joins\"><a href=\"#contents\"><font size=\"4\" color=\"red\"><b>JOINS</b></font></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More often that not, you will need to get data from multiple datasets. \"JOIN\"s in SQL help you do just that. Let's create another dataset in pandas to show you how different joins work in sql. Joins are one of the most commonly asked topic in any SQL interview. So make sure you thoroughly understand how different joins work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.DataFrame({'code':['S', 'C', 'L'],'city':['Southampton', 'Cherbourg', 'London']})\n",
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw above, in 'Embarked' column of \"titanic_disaster_dataset\" there are three codes \"S\", \"C\" and \"Q\". There \"S\" and \"C\" are the common values between \"cities\" and \"titanic_disaster_dataset\", but \"L\"(present only in \"cities\") and \"Q\"(present only in \"titanic_disaster_dataset\") are present in one of them but not in another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>IMPORTANT</i></b><br>\n",
    "Before joining any two tables, it is imperative that you know the levels/columns at which the data is unique in both the tables. For example, the titanic dataset above is unique at \"PassengerId\" column and cities data is unique at \"code\" column level. Not understanding the uniqueness of the data is a leading cause of incorrect results in SQL. Being mindful of the levels in the data will ensure you are making meaningful joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>INNER JOIN</b><br>\n",
    "INNER JOIN gives rows corresponding to only the common values(in the columns used in the inner join condition) between the two tables\n",
    "<img src = \"https://www.w3schools.com/sql/img_innerjoin.gif\" width=\"25%\" height=\"25%\"><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    c.city,\n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    AVG(df.Age) AS avg_age,\n",
    "    SUM(df.Age) AS tot_age,\n",
    "    MIN(df.Age) AS min_age,\n",
    "    MAX(df.Age) AS max_age,\n",
    "    COUNT(df.PassengerId) AS tot_passengers,\n",
    "    COUNT(DISTINCT df.Ticket) AS dist_tickets\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "INNER JOIN cities AS c\n",
    "    ON df.Embarked = c.code\n",
    "\n",
    "GROUP BY df.Embarked,\n",
    "    df.Sex\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using INNER JOIN we get only the common elements - \"S\" and \"C\", but not \"L\"(present only in \"cities\") and \"Q\"(present only in \"titanic_disaster_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>LEFT JOIN</b><br>\n",
    "LEFT JOIN gives all rows from the left table and only those rows from right table where the values are common in the columns used in left join condition\n",
    "<img src = \"https://www.w3schools.com/sql/img_leftjoin.gif\" width=\"25%\" height=\"25%\"><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    c.city,\n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    AVG(df.Age) AS avg_age,\n",
    "    SUM(df.Age) AS tot_age,\n",
    "    MIN(df.Age) AS min_age,\n",
    "    MAX(df.Age) AS max_age,\n",
    "    COUNT(df.PassengerId) AS tot_passengers,\n",
    "    COUNT(DISTINCT df.Ticket) AS dist_tickets\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "LEFT JOIN cities AS c\n",
    "    ON df.Embarked = c.code\n",
    "\n",
    "GROUP BY df.Embarked,\n",
    "    df.Sex\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using LEFT JOIN we get all the rows from the left table(\"S\", \"Q\", \"C\" - all are present in the output), but only for \"C\" and \"S\", we are getting \"city\" from the right table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>You will mostly be using INNER and LEFT JOINs in your career, but it is good to know about other joins too<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>RIGHT JOIN</b><br>\n",
    "RIGHT JOIN is the mirror opposite of LEFT JOIN. RIGHT JOIN gives all rows from the right table and only those rows from left table where the values are common in the columns used in right join condition.\n",
    "<img src = \"https://www.w3schools.com/sql/img_rightjoin.gif\" width=\"25%\" height=\"25%\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately \"RIGHT JOIN\" is not supported by pandasql. But we can see the output of RIGHT JOIN by swapping the tables in the LEFT JOIN query above. Here is how it will look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    c.city,\n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    AVG(df.Age) AS avg_age,\n",
    "    SUM(df.Age) AS tot_age,\n",
    "    MIN(df.Age) AS min_age,\n",
    "    MAX(df.Age) AS max_age,\n",
    "    COUNT(df.PassengerId) AS tot_passengers,\n",
    "    COUNT(DISTINCT df.Ticket) AS dist_tickets\n",
    "    \n",
    "FROM cities AS c\n",
    "\n",
    "LEFT JOIN titanic_disaster_dataset AS df\n",
    "    ON df.Embarked = c.code\n",
    "\n",
    "GROUP BY df.Embarked,\n",
    "    df.Sex\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>FULL JOIN</b><br>\n",
    "FULL JOIN gives all rows from the from both tables, irrespective of common values\n",
    "<img src = \"https://www.w3schools.com/sql/img_fulljoin.gif\" width=\"25%\" height=\"25%\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately \"FULL JOIN\" is not supported by pandasql. But here is how the output will look like if you ran a FULL join on the two datasets we have - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample output for FULL JOIN\n",
    "pd.merge(titanic_disaster_dataset.groupby('Embarked')['Age'].mean().reset_index(), \n",
    "         cities.rename({'code':'Embarked'}, axis = 'columns'), \n",
    "         on='Embarked', \n",
    "         how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the output has rows corresponding to both common and non-common elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id=\"adv\"><a href=\"#contents\"><font size=\"4\" color=\"red\"><b>ADVANCED</b></font></a></div></a></div>\n",
    "<ol type=\"square\">\n",
    "    <li><a href=\"#adv_union\">Union</a></li>\n",
    "    <li><a href=\"#adv_minus\">Minus</a></li>\n",
    "    <li><a href=\"#adv_rank\">Ranking in SQL</a></li>\n",
    "    <li><a href=\"#adv_ll\">Lead and Lag</a></li>\n",
    "    <li><a href=\"#adv_sub\">Subqueries</a></li>\n",
    "    <li><a href=\"#adv_type\">Typecast</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id =\"adv_union\"> <a href=\"#adv\"><font size=\"3\" color=\"black\"><b>Union and Union All</b></font></a></div>\n",
    "If you have two datasets with exact same columns and you want to stack one on top of another and get all the distinct rows, you can use \"UNION\" statement. Each SELECT statement within the UNION must have the same number of fields in the result sets with similar data types, as shown below -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using \"UNION\" to stack datasets\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT DISTINCT df.Embarked FROM titanic_disaster_dataset AS df\n",
    "UNION\n",
    "SELECT DISTINCT c.code FROM cities AS c\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"UNION\" gives you only the distinct rows after stacking. If you do not want distinct rows, you can use \"UNION ALL\", which will simply stack the two tables one over another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using \"UNION ALL\" to stack datasets\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT DISTINCT df.Embarked FROM titanic_disaster_dataset AS df\n",
    "UNION ALL\n",
    "SELECT DISTINCT c.code FROM cities AS c\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id =\"adv_minus\"> <a href=\"#adv\"><font size=\"3\" color=\"black\"><b>Minus or Except</b></font></a></div>\n",
    "While \"UNION\" combines the datasets, \"MINUS\" or \"EXCEPT\" (only in some SQL dialects) gives the rows from the first dataset which are not present in the second one. pandasql uses \"EXCEPT\" keyword. Here are two examples -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using \"EXCEPT\" to take rows from first dataset and then remove rows which are present in both datasets\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT DISTINCT df.Embarked FROM titanic_disaster_dataset AS df\n",
    "EXCEPT\n",
    "SELECT DISTINCT c.code FROM cities AS c\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get \"None\"(i.e. NULL) and \"Q\" as output because these two values are present in the first dataset and not the second. Here is the output if we take \"cities\" as the first dataset instead - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using \"EXCEPT\" to take rows from first dataset and then remove rows which are present in both datasets\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT DISTINCT c.code FROM cities AS c\n",
    "EXCEPT\n",
    "SELECT DISTINCT df.Embarked FROM titanic_disaster_dataset AS df\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id =\"adv_rank\"> <a href=\"#adv\"><font size=\"3\" color=\"black\"><b>Ranking in SQL</b></font></a></div>\n",
    "There are lot times when you would want to sort your data and give a rank to each row in the dataset. RANK(), DENSE_RANK() and ROW_NUMBERS() are three functions which help you do just that. Generally these functions are used with OVER() function to sort the data based on some column before giving rank to each row and also to partition the data, which we will learn later. Let's explore RANK() first.<br><br>\n",
    "RANK() provides a rank to each row. If any n rows have the same value in the sorting column, then RANK() gives the same value to all of these rows. For the next row where the value is different than the previous n rows, the rank given will be previous rank plus n. Here is an example - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RANK() sorts the data, gives the same rank to all consecutive n rows with same values, \n",
    "#but for next row with a different value, it gives a rank equal to previous rank plus n\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Name,\n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    df.Age,\n",
    "    df.Fare,\n",
    "    RANK() OVER(ORDER BY df.Age DESC) AS rnk\n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, \"OVER(ORDER BY df.Age DESC)\" orders Age in descending order and then Rank() is used to assign a rank to each row. Notice row 3 and 4, where the age values are the same(i.e. 71) and hence both rows have the same rank(=3). In row 5, the ranking begins from 5(=previous rank 3 plus 2) and not 4.<br><br>\n",
    "\n",
    "So by using RANK() there are some ranks which get skipped if there are same values in the sorting order. Using DENSE_RANK() we can ensure that no ranks are skipped. Notice in the example below where row 5 has a rank 4 and not 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DENSE_RANK() does not skip ranks like RANK() does\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Name,\n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    df.Age,\n",
    "    df.Fare,\n",
    "    DENSE_RANK() OVER(ORDER BY df.Age DESC) AS rnk\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we have ROW_NUMBER() which gives different rank to each row even if the sorted values are same. Notice below how in row 3 and 4 age is same but the ranks are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ROW_NUMBER() sorts the data and gives different ranks to each row \n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Name,\n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    df.Age,\n",
    "    df.Fare,\n",
    "    ROW_NUMBER() OVER(ORDER BY df.Age DESC) AS rnk\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DENSE_RANK() is the most often used function among these three. But depending on your use-case you can choose any of the three functions above. \n",
    "\n",
    "So far we used only one column to sort the data, we can use use multiple columns to sort the data too. We can also specify in what order we want to sort the data. Here is an example where we are sorting the data first with Age, and if Age is same then sorting with Fare - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#You can use multiple columns to sort the data within OVER() function\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Name,\n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    df.Age,\n",
    "    df.Fare,\n",
    "    RANK() OVER(ORDER BY df.Age DESC, df.Fare ASC) AS rnk\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, OVER() also supports partitioning of data. Suppose you want to give separate ranks withtin specific levels of data, then you can use \"PARTITION BY\" in OVER() function. In the example below, for each combination of df.Embarked and df.Sex, we will get different sets of ranks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using PARTITION BY to create buckets for giving separate sets of ranks\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Name,\n",
    "    df.Embarked,\n",
    "    df.Sex,\n",
    "    df.Age,\n",
    "    df.Fare,\n",
    "    RANK() OVER(PARTITION BY df.Embarked, df.Sex \n",
    "                ORDER BY df.Age DESC, df.Fare DESC) AS rnk\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id =\"adv_ll\"> <a href=\"#adv\"><font size=\"3\" color=\"black\"><b>Lead and Lag</b></font></a></div>\n",
    "LEAD() and LAG() can be used to get the nth previous or nth next value in a column, after sorting the data using OVER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examples for LEAD() and LAG()\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Name,\n",
    "    LEAD(df.Name, 1) OVER(ORDER BY df.Name) AS next_name,\n",
    "    LEAD(df.Name, 2) OVER(ORDER BY df.Name) AS second_next_name,\n",
    "    LAG(df.Name, 1) OVER(ORDER BY df.Name) AS previous_name,\n",
    "    LAG(df.Name, 2) OVER(ORDER BY df.Name) AS second_previous_name\n",
    "    \n",
    "FROM titanic_disaster_dataset AS df\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using \"PARTITION BY\" within OVER() function, you can partition the data and get LEAD() and LAG() of a column within partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id =\"adv_sub\"> <a href=\"#adv\"><font size=\"3\" color=\"black\"><b>Subqueries</b></font></a></div>\n",
    "Subqueries are small queries which are used within a bigger main SQL query. Subqueries store the output of a SQL temporarily and more SQL queries can be written on top of these.<br>\n",
    "\n",
    "Here are some examples of Subquery -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using subqueries to get average age and then filtering only those rows where average age is below 30\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "\n",
    "    df1.Embarked,\n",
    "    df1.avg_age\n",
    "    \n",
    "FROM (SELECT \n",
    "            a.Embarked, \n",
    "            AVG(a.Age) AS avg_age \n",
    "        FROM titanic_disaster_dataset AS a\n",
    "        GROUP BY a.Embarked) AS df1\n",
    "\n",
    "WHERE df1.avg_age<30\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here df1 is a temporary datasest which gets created via a subquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using subqueries to get average age and number of passengers separately and then joining the outputs\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df1.Embarked,\n",
    "    df1.avg_age,\n",
    "    df2.tot_passengers\n",
    "FROM (SELECT \n",
    "            a.Embarked, \n",
    "            AVG(a.Age) AS avg_age \n",
    "        FROM titanic_disaster_dataset AS a\n",
    "        GROUP BY a.Embarked) AS df1\n",
    "\n",
    "INNER JOIN (SELECT \n",
    "                b.Embarked, \n",
    "                COUNT(b.PassengerID) AS tot_passengers\n",
    "            FROM titanic_disaster_dataset AS b\n",
    "            GROUP BY b.Embarked) AS df2\n",
    "    ON df1.Embarked = df2.Embarked\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can nest subqueries inside other subquery and also use them to filter dataset as seen below -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using nested subqueries to filter data\n",
    "ps.sqldf(\"\"\"\n",
    "SELECT b.* FROM titanic_disaster_dataset AS b\n",
    "WHERE b.Embarked IN \n",
    "       (SELECT \n",
    "            DISTINCT df1.Embarked\n",
    "        FROM (SELECT \n",
    "                    a.Embarked, \n",
    "                    AVG(a.Age) AS avg_age \n",
    "                FROM titanic_disaster_dataset AS a\n",
    "                GROUP BY a.Embarked) AS df1\n",
    "        WHERE df1.avg_age>30)\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"left\" id =\"adv_type\"> <a href=\"#adv\"><font size=\"3\" color=\"black\"><b>Typecast</b></font></a></div>\n",
    "You can use CAST() function to change the data type of columns in a dataset. Here are some example - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using CAST() to change datatype of columns\n",
    "ps.sqldf(\"\"\"\n",
    "\n",
    "SELECT \n",
    "    df.Name,\n",
    "    df.Fare,\n",
    "    CAST(df.Fare AS INT) AS integer_fare,\n",
    "    CAST(CAST(df.Fare AS INT) AS FLOAT) AS float_fare\n",
    "FROM titanic_disaster_dataset AS df\n",
    "LIMIT 5\n",
    "    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are any numbers stored as string, you can cast them to INT/FLOAT. You can also change numbers to string, if your use-case requires that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>So what next?</b><br>\n",
    "Now that you have learned the basics of SQL, it is important to practice what you have learned so that it sticks. Remember SQL is the most basic and must-have skill for any data analyst. I, therefore, recommend you to spend some time in doing the SQL assignment which comes with this module or use practise on any one of these websites - \n",
    "<ol>\n",
    "    <li><a href=\"https://sqlzoo.net/\">sqlzoo</a></li>\n",
    "    <li><a href=\"https://www.hackerrank.com/domains/sql\">HackerRank</a></li>    \n",
    "</ol>\n",
    "Some do's and don'ts -\n",
    "<ol>\n",
    "    <li>Before using any dataset, thoroughly understand what is the level of the data and what kind of information is present in each column of the data</li>\n",
    "    <li>NEVER Join two tables unless you know the level of data in both the tables</li>\n",
    "    <li>To make life easier for every one, format your queries in a neat order. I personally use <a href=\"https://sqlformat.org/\">sqlformat.org</a> to make my queries prettier</li>\n",
    "    <li>Use google and stackoverflow extensively - No human has answers for everything, but there is a good chance that some random dude on Google or Stackoverflow knows the answer to your query or how to fix your code error</li>\n",
    "</ol>\n",
    "\n",
    "Feel free to ping me on <a href=\"https://www.linkedin.com/in/utsavawasthi/\">LinkedIn</a> and subscribe to my <a href=\"https://www.youtube.com/channel/UCJpp55YcP5swWj8TmIOd0OA/\">YouTube channel</a>.<br><br>\n",
    "\n",
    "And lastly, remember this :) -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 10 color=\"green\">\"</font><br>\n",
    "<div align = center> \n",
    "    <font size = 5 color=\"green\" style=\"font-family: 'Comic Sans MS'\">Pass on what you have learned</font><br>\n",
    "</div>\n",
    "<div align = right> \n",
    "    <font size = 10 color=\"green\"t>\"</font><br>\n",
    "</div>\n",
    "<img align = \"right\" src = \"https://orig00.deviantart.net/ff5e/f/2013/102/4/4/yoda_by_valval-d61g9rh.png\" width=\"20%\" height=\"20%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
